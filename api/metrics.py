from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
import time

REQUEST_COUNT = Counter(
    'api_requests_total',
    'Total number of API requests',
    ['method', 'endpoint', 'status_code']
)

REQUEST_DURATION = Histogram(
    'api_request_duration_seconds',
    'API request duration in seconds',
    ['method', 'endpoint']
)

ACTIVE_REQUESTS = Gauge(
    'api_active_requests',
    'Number of active API requests'
)

LLAMA_GENERATION_DURATION = Histogram(
    'llama_generation_duration_seconds',
    'Time spent generating responses from llama.cpp',
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

LLAMA_TOKENS_GENERATED = Counter(
    'llama_tokens_generated_total',
    'Total number of tokens generated by llama.cpp'
)

LLAMA_TOKENS_PER_SECOND = Histogram(
    'llama_tokens_per_second',
    'Tokens generated per second by llama.cpp',
    buckets=[1, 5, 10, 20, 50, 100, 200]
)

LLAMA_CONTEXT_SIZE = Histogram(
    'llama_context_size_tokens',
    'Size of context sent to llama.cpp in tokens',
    buckets=[100, 500, 1000, 2000, 4000, 8000]
)

class MetricsMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        if request.url.path == "/metrics":
            return await call_next(request)
        
        start_time = time.time()
        ACTIVE_REQUESTS.inc()
        
        try:
            response = await call_next(request)
            
            duration = time.time() - start_time
            
            REQUEST_COUNT.labels(
                method=request.method,
                endpoint=request.url.path,
                status_code=response.status_code
            ).inc()
            
            REQUEST_DURATION.labels(
                method=request.method,
                endpoint=request.url.path
            ).observe(duration)
            
            return response
            
        finally:
            ACTIVE_REQUESTS.dec()

metrics_middleware = MetricsMiddleware

def record_llama_metrics(generation_time: float, tokens_generated: int, tokens_per_second: float, context_tokens: int = 0):
    LLAMA_GENERATION_DURATION.observe(generation_time)
    LLAMA_TOKENS_GENERATED.inc(tokens_generated)
    LLAMA_TOKENS_PER_SECOND.observe(tokens_per_second)
    if context_tokens > 0:
        LLAMA_CONTEXT_SIZE.observe(context_tokens)

def get_metrics():
    return Response(
        content=generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )