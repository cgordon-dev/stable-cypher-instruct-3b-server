# Stable Cypher Instruct 3B Configuration
# Copy this file to .env and configure as needed

# ================================
# Model Configuration
# ================================
MODEL_FILENAME=stable-cypher-instruct-3b.Q4_K_M.gguf

# ================================
# Deployment Mode (cpu or gpu)
# ================================
DEPLOYMENT_MODE=cpu

# ================================
# Server Configuration
# ================================

# CPU-specific settings (used when DEPLOYMENT_MODE=cpu)
CPU_THREADS=8
LLAMA_PORT_CPU=8080

# GPU-specific settings (used when DEPLOYMENT_MODE=gpu)
GPU_LAYERS=32
LLAMA_PORT_GPU=8081

# Model parameters
CONTEXT_SIZE=4096
MAX_TOKENS=512
TEMPERATURE=0.7
TOP_P=0.9

# ================================
# API Configuration
# ================================
API_PORT=8000
API_KEY=
LOG_LEVEL=INFO

# Request handling
REQUEST_TIMEOUT=30
MAX_RETRIES=3

# ================================
# Monitoring Configuration
# ================================
ENABLE_METRICS=true
METRICS_PORT=8001

PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
GRAFANA_PASSWORD=admin

# Monitoring exporters
NODE_EXPORTER_PORT=9100
CADVISOR_PORT=8082
DCGM_EXPORTER_PORT=9400

# ================================
# Web UI Configuration
# ================================
WEB_PORT=5001
WEB_SECRET_KEY=dev-secret-key-change-in-production
METRICS_UPDATE_INTERVAL=5

# ================================
# Advanced Configuration
# ================================

# Llama.cpp server endpoint (usually auto-configured)
# LLAMA_ENDPOINT=http://llama-server-cpu:8080

# Additional environment variables for fine-tuning
# LLAMA_DEBUG=0
# LLAMA_CACHE_PROMPT=true
# LLAMA_NUMA=false